<html><head><title>Model Comparison Report</title><style>body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; } table { border-collapse: collapse; width: 100%; margin-bottom: 20px; box-shadow: 0 2px 3px rgba(0,0,0,0.1); } th, td { padding: 10px 12px; text-align: left; border: 1px solid #ddd; } th { background-color: #f2f2f2; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } .lda { background-color: #e6f3ff; } .bert { background-color: #ffe6e6; } h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; } h2 { color: #34495e; border-bottom: 1px solid #bdc3c7; padding-bottom: 5px; margin-top: 30px; } h3 { color: #7f8c8d; margin-top: 20px; } img { max-width: 80%; height: auto; display: block; margin: 15px auto; border: 1px solid #ddd; padding: 5px; box-shadow: 0 2px 3px rgba(0,0,0,0.1); } .section { margin-bottom: 30px; background-color: #fff; padding: 20px; border-radius: 5px; box-shadow: 0 1px 2px rgba(0,0,0,0.05); } ul { padding-left: 20px; } li { margin-bottom: 8px; } a { color: #3498db; text-decoration: none; } a:hover { text-decoration: underline; } .code { background-color: #ecf0f1; padding: 2px 5px; border-radius: 3px; font-family: monospace; } .value-positive { color: #27ae60; } .value-negative { color: #c0392b; } .value-neutral { color: #7f8c8d; }</style></head><body><h1>Model Comparison Report: LDA vs BERTopic</h1><div class='section'><h2>Introduction</h2><p>This report compares the performance and results of two topic modeling approaches applied to the same dataset:</p><ul><li><strong>LDA (Latent Dirichlet Allocation):</strong> A traditional probabilistic topic modeling method focused on word co-occurrences.</li><li><strong>BERTopic:</strong> A modern approach leveraging transformer-based sentence embeddings and clustering techniques.</li></ul><p>The comparison focuses on user topic distributions, suspicious user detection, topic coherence (if available), and qualitative topic assessment.</p></div><div class='section'><h2>Model Configuration</h2><table><thead><tr><th>Parameter</th><th class='lda'>LDA Config</th><th class='bert'>BERTopic Config</th></tr></thead><tbody></tbody></table></div><div class='section'><h2>Performance & Results Comparison</h2><h3>Topic Coherence</h3><p>Topic coherence measures the semantic interpretability of topics based on their top words. Higher scores are generally better. <i>(Note: Coherence calculation methods might differ between models.)</i></p><table><thead><tr><th>Model</th><th>Mean Coherence Score</th><th>Difference (BERTopic - LDA)</th><th>% Improvement vs LDA</th></tr></thead><tbody><tr><td class='lda'>LDA</td><td>0.4513</td><td rowspan='2'>0.0923</td><td rowspan='2'>20.45%</td></tr><tr><td class='bert'>BERTopic</td><td>0.5436</td></tr></tbody></table><img src='coherence_comparison.png' alt='Coherence Comparison Chart'><h3>User Topic Narrowness Correlation</h3><p>This section compares how similarly the two models assess user topic concentration (narrowness/diversity) using various metrics. High correlation suggests agreement between models.</p><table><thead><tr><th>Metric</th><th>Correlation (LDA vs BERTopic)</th><th>LDA Mean ± Std Dev</th><th>BERTopic Mean ± Std Dev</th><th>Mean Difference (BERTopic - LDA)</th></tr></thead><tbody><tr><td>Gini Coefficient</td><td>0.183</td><td class='lda'>0.734 ± 0.098</td><td class='bert'>0.846 ± 0.035</td><td>+0.113</td></tr><tr><td>Shannon Entropy</td><td>0.255</td><td class='lda'>1.895 ± 0.577</td><td class='bert'>0.816 ± 0.394</td><td>-1.079</td></tr><tr><td>Top1 Ratio</td><td>0.192</td><td class='lda'>0.532 ± 0.175</td><td class='bert'>0.858 ± 0.108</td><td>+0.326</td></tr></tbody></table><img src='narrowness_metrics_comparison.png' alt='Narrowness Metrics Comparison Scatter Plots'><h3>Suspicious User Detection</h3><p>This compares the sets of users flagged as potentially anomalous (e.g., bots, spammers) by each model based on posting frequency, topic narrowness, and content duplication.</p><table><thead><tr><th>Metric</th><th>Value</th></tr></thead><tbody><tr><td>Total Suspicious Users (LDA)</td><td>438</td></tr><tr><td>Total Suspicious Users (BERTopic)</td><td>448</td></tr><tr><td>Users Flagged by Both (Overlap)</td><td>433</td></tr><tr><td>Overlap % of LDA Total</td><td>98.9%</td></tr><tr><td>Overlap % of BERTopic Total</td><td>96.7%</td></tr><tr><td>Users Flagged by LDA Only</td><td>5</td></tr><tr><td>Users Flagged by BERTopic Only</td><td>15</td></tr></tbody></table><img src='suspicious_users_overlap.png' alt='Suspicious Users Overlap Venn Diagram'><p>See <a href='suspicious_user_discrepancies.csv'>suspicious_user_discrepancies.csv</a> for users flagged by only one model.</p></div><div class='section'><h2>Topic Word & Similarity Analysis</h2><p>View the detailed side-by-side topic word lists and similarity analysis: <a href='topic_words_comparison.html'>Topic Words Comparison Report</a></p><h3>Most Similar Topic Pairs (Jaccard > 0.1)</h3><p>Top pairs of topics from LDA and BERTopic based on shared words (calculated using top 20 words per topic).</p><table><thead><tr><th>LDA Topic ID</th><th>BERTopic Topic ID</th><th>Similarity Score</th><th>Number Shared Words</th><th>Example Shared Words</th></tr></thead><tbody><tr><td class='lda'>4</td><td class='bert'>1</td><td>0.304</td><td>7</td><td>der, die, für, ist, merkel</td></tr><tr><td class='lda'>5</td><td class='bert'>0</td><td>0.200</td><td>5</td><td>clinton, hillary, like, people, trump</td></tr><tr><td class='lda'>1</td><td class='bert'>2</td><td>0.154</td><td>4</td><td>brussels, islamkills, prayforbrussels, stopislam</td></tr><tr><td class='lda'>3</td><td class='bert'>0</td><td>0.154</td><td>4</td><td>dont, get, like, people</td></tr><tr><td class='lda'>6</td><td class='bert'>0</td><td>0.111</td><td>3</td><td>dont, politics, trump</td></tr><tr><td class='lda'>7</td><td class='bert'>0</td><td>0.111</td><td>3</td><td>clinton, politics, trump</td></tr><tr><td class='lda'>9</td><td class='bert'>0</td><td>0.111</td><td>3</td><td>dont, get, people</td></tr><tr><td class='lda'>9</td><td class='bert'>2</td><td>0.111</td><td>3</td><td>gun, gunsny, prayerscalifornia</td></tr><tr><td class='lda'>11</td><td class='bert'>0</td><td>0.111</td><td>3</td><td>get, like, people</td></tr></tbody></table></div><div class='section'><h2>Conclusions & Observations</h2><ul><li>BERTopic produced significantly more coherent topics than LDA according to the calculated metric, suggesting better semantic interpretability with the embedding-based approach for this dataset.</li><li>Weak positive correlation in user topic narrowness; the models seem to provide different perspectives on user behavior concentration.</li><li>High agreement (% overlap > 60%) between models on suspicious user detection, suggesting a core set of anomalous users are identified consistently.</li><li>Overall, BERTopic appears to offer advantages in topic quality (coherence) for this specific dataset run, while narrowness agreement varies. BERTopic's embedding approach may capture nuances missed by LDA.</li></ul></div><div class='section' style='text-align: center; font-size: 0.9em; color: #7f8c8d;'><p><em>Report generated on 2025-04-29 19:27:51</em></p></div></body></html>